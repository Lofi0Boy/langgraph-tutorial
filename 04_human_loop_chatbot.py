from typing import Annotated

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_tavily import TavilySearch
from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.types import Command, interrupt
from langchain_core.tools import tool, InjectedToolCallId
from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, AIMessage
from typing import Optional

load_dotenv()

class State(TypedDict):
    messages: Annotated[list, add_messages]
    user_name: Optional[str]
    conversation_purpose: Optional[str]


@tool
def update_user_info( tool_call_id: Annotated[str,InjectedToolCallId], name: str = None, purpose: str = None) -> Command|str:
    '''
    Use this tool to update the name of the user or purpose of the chat.
    '''
    result_parts = []
    new_state = {}
    if name:
        new_state['user_name'] = name
        result_parts.append(f"ì´ë¦„ì„ {name}ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤")
    if purpose:
        new_state['conversation_purpose'] = purpose
        result_parts.append(f"ëŒ€í™” ëª©ì ì„ {purpose}ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤")
    
    if result_parts:
        # Commandë¡œ ë°˜í™˜í•˜ë©´ ToolNodeì—ì„œ Commandë¥¼ ì²˜ë¦¬í•œë‹¤.
        # ToolNodeì—ì„œ ì´ê²Œ ì–´ë–¤ íˆ´ì½œì˜ ê²°ê³¼ì¸ì§€ ì•Œê¸° ìœ„í•´ì„œ tool_call_idë¥¼ ì „ë‹¬í•˜ëŠ”ë“¯
        # tool_call_idì²˜ëŸ¼ langgraphê°€ ìë™ìœ¼ë¡œ ì£¼ì…í•´ì£¼ëŠ” ê°’ì€ ê¸°ë³¸ê°’ ì„¤ì • X
        new_state['messages'] = [ToolMessage(content=result_parts,tool_call_id=tool_call_id)]
        return Command(update=new_state)
    else:
        return "ì‚¬ìš©ì ë°ì´í„°í„°ë¥¼ ì—…ë°ì´íŠ¸ í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

# tool ë¡œì„œê°€ ì•„ë‹ˆë¼ Nodeë¡œì„œ ë‹¤ë¤„ë³´ì. Toolë¡œì„œ ì‚¬ìš©í•˜ê¸°ì—” ë­”ê°€ ì´ìƒí•œë“¯,
def human_assistance(state: State) -> State:
    """Request assistance from a human."""
    human_response = interrupt({"query": state["messages"][-1].content})
    return human_response["data"]

def chatbot(state: State):
    
    global current_llm, llm_with_tools
    
    # í˜„ì¬ ëª¨ë¸ë¶€í„° ì‹œì‘í•´ì„œ ëª¨ë“  ëª¨ë¸ì„ ì‹œë„
    available_models = list(chat_models.keys())
    tried_models = []
    
    # í˜„ì¬ ëª¨ë¸ì„ ë¨¼ì € ì‹œë„
    if current_llm in available_models:
        available_models.remove(current_llm)
        available_models.insert(0, current_llm)
    
    for model in available_models:
        try:
            llm_with_tools = chat_models[model].bind_tools(tools)
            message = llm_with_tools.invoke(state["messages"])
            assert len(message.tool_calls) <= 1
            current_llm = model  # ì„±ê³µí•œ ëª¨ë¸ì„ í˜„ì¬ ëª¨ë¸ë¡œ ì„¤ì •
            return {"messages": [message]}
        except Exception as e:
            tried_models.append(model)
            print(f"Model {model} failed: {str(e)}")
            continue
    
    # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš°
    error_message = AIMessage(content="Sorry. All the models are not working. Try later.")
    return {"messages": [error_message]}

tavily = TavilySearch(max_result=2)
tools = [tavily, update_user_info]

claude = init_chat_model("anthropic:claude-3-5-sonnet-latest")
openai = init_chat_model("openai:gpt-4o-mini")
gemini = init_chat_model("google_genai:gemini-2.0-flash")
chat_models = {'gemini':gemini, 'claude':claude, 'openai':openai}

current_llm = 'gemini'
llm_with_tools = chat_models[current_llm].bind_tools(tools)


memory = InMemorySaver()

tool_node = ToolNode(tools)

graph_builder = StateGraph(State)
graph_builder.add_node("chatbot", chatbot)
graph_builder.add_node("tools", tool_node)

graph_builder.add_edge(START, "chatbot")
graph_builder.add_conditional_edges("chatbot", tools_condition)
graph_builder.add_edge("tools", "chatbot")

graph = graph_builder.compile(checkpointer=memory)

def stream_graph_updates(input: str|Command, config: dict):
    if isinstance(input, Command):
        events = graph.stream(input, config, stream_mode='values')
    else:
        events = graph.stream({'messages': [HumanMessage(content=input)]}, config, stream_mode='values')


    for i,event in enumerate(events):
        print('step',i)
        if 'messages' in event and event['messages']:
            message = event['messages'][-1]
            message.pretty_print()



config = {'configurable': {'thread_id': '123'}}

while True:
    # stateSnapshot => Value, Interrupts, next ë“±ì„ ê°€ì§€ê³  ìˆìŒ. 
    # ë˜ ë­˜ ê°€ì§€ê³  ìˆëŠ”ì§€ëŠ” ë‚˜ì¤‘ì— ì•Œê²Œ ë˜ê² ì§€?
    current_state = graph.get_state(config)
    if current_state.interrupts :
        for interrupt in current_state.interrupts:
            query = interrupt.value['query']
            human_response = input(f"User: {query}")
            stream_graph_updates(Command(resume={"data": human_response}),config)
            break
        continue

    user_input = input('You: ')
    if user_input.lower() in ['q', 'quit', 'exit']:
        print('ğŸ‘‹ Bye!')
        break
        
    stream_graph_updates(user_input, config)






